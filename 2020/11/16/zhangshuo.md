原文 ：https://netflixtechblog.com/building-netflixs-distributed-tracing-infrastructure-bb856c319304

为了解决特定流媒体故障，需要重建流 session，这个过程是繁琐耗时的，分布式 trace 系统就是来解决这一问题。

### 架构

![架构](https://miro.medium.com/max/4800/0*KZD1JGqoK57fE2jY)

分布式 trace 基础设施分为三个部分：跟踪工具库、流处理和存储。

### 工具库
- tracer 库主要问题是怎么传输上下文。
- 这里采用的是 Open-Zipkin 的 B3 HTTP(1) 请求头的上下文传播机制。注入了像 服务名称、自动伸缩组(ASG) 和容器实例标识符等基础设施标签，通过这些标签来查询带有日志数据的 trace。

### 流处理
1. 流处理最重要的问题是数据采样策略。
- 宽松的采样策略会导致性能等问题，还需要可拓展的流处理和存储设施。大量取样又无法保证故障诊断的可靠。
- 这里采用混合采样，允许为特定的、可配置的请求集 100% 采样，同时根据摄取点的策略进行随机采样。

2. 第二个问题是通过可拓展的数据处理平台来处理大量的 trace 数据。
- 先将数据传输到作业集群，然后缓存一个时间段的 span，以便在一个作业中收集一个 trace 的所有 span
- 下一个作业从上一个作业中获取数据源，对数据进行尾部采样，然后将 trace 写入存储系统。

### 存储
存储最大的问题是对于大数据量，存储成本和查询速度。
最初采用 ES 存储，但高数据写入率导致 ES 集群负担增加，ES 要使用大量计算资源为新增的 trace 创建索引。
后来迁移到 Cassandra，横向拓展可以处理更高的写入率，但成本高。通过以下手段来优化
1. 使用更便宜的 EBS 而不是 SSD，优化时间窗口压缩策略(TWCS)参数，减少 Cassandra SSTable 文件的磁盘写入和合并操作。
2. 在 Cassandra 数据文件上启用 Zstd 块压缩，减少 trace 数据大小。
3. 通过检查 trace 所有 span 的warn、error 和 retry 标签，标记 trace【看起来是个优化的方向，根据标签分析 trace，健康的 trace 不采】

### 其他收益
#### 应用程序健康检测
* 通过 trace 中的关联信息推断微服务拓扑，并将 trace 与 Atlas 的时间序列数据关联，描绘出更丰富的应用健康状况的可观察性画像
#### 容错工程
* 通过 trace 验证故障是否被正确注入
#### 地域容灾
* trace 提供了微服务交互的设备类型的可见性
#### A/B 测试
* 通过分析 A/B 测试的 trace 信息，评估测试效果

### 下一步
1. 推广 trace，提供通过标记相关的元数据为每个请求提供额外上下文的能力
2. 增强 trace 的分析能力，针对特定用例构建自己的仪表盘和系统
3. 将 metrics、logging 和 trace 的数据关联起来


## 个人总结
删减了一些，看起来还是记录了一堆细节，感觉这个度不太好把握，因为有些细节我之前也不了解，感觉是有必要记录的
